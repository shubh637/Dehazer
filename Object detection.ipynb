{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e31258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1681997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API\n",
    "#importing the model and file\n",
    "config_file=\"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n",
    "frozen_model=\"C:\\\\Users\\\\TUSHAR SAIN\\\\Desktop\\\\models\\\\Real-Time-Object-Detection-With-OpenCV-master\\\\frozen_inference_graph.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cv2.dnn_DetectionModel(frozen_model,config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae65a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabels=[]\n",
    "file_name=\"Labels.txt\"\n",
    "with open(file_name,\"rt\") as fpt:\n",
    "    classLabels=fpt.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe679a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(classLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setInputSize(320,320)\n",
    "model.setInputScale(1.0/127.5)\n",
    "model.setInputMean((127.5,127.5,127.5))\n",
    "model.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a39044",
   "metadata": {},
   "source": [
    "Read an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(\"./557-5579615_man-and-car-png-png-download-man-and.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)##bgr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14175aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classindex,confidence,bbox=model.detect(img,confThreshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Classindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95962bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_scale=3\n",
    "font=cv2.FONT_HERSHEY_PLAIN\n",
    "for ClassInd,conf,boxes in zip(Classindex.flatten(),confidence.flatten(),bbox):\n",
    "#     cv2.rectangle(frame,(x,y,(x+w,y+h),(255,0,0),2)\n",
    "#     cv2.putText(img,text,(text_offest_x,text_offset_y),font,fontScale=font_scale,color=(0,0,0),thickness=1)\n",
    "    cv2.rectangle(img,boxes,(25,0,0),2)\n",
    "    cv2.putText(img,classLabels[ClassInd-1],(boxes[0]+10,boxes[1]+40),font,fontScale=font_scale,color=(0,255,0),thickness=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36b1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bed33",
   "metadata": {},
   "source": [
    "Video Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "#Check id the video is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video\")\n",
    "\n",
    "font_scale=3 \n",
    "font=cv2.FONT_HERSHEY_PLAIN\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    ClassIndex,confidence,bbox=model.detect(frame,confThreshold=0.55)\n",
    "    print(ClassIndex)\n",
    "    if(len(Classindex)!=0):\n",
    "        for ClassInd,conf,boxes in zip(ClassIndex.flatten(),confidence.flatten(),bbox):\n",
    "            if (ClassInd<=80):\n",
    "                cv2.rectangle(frame,boxes,(255,0,0),2)\n",
    "                cv2.putText(frame,classLabels[ClassInd-1],(boxes[0]+10,boxes[1]+40),font,fontScale=font_scale,color=(0,255,0),thickness=3)\n",
    "    cv2.imshow(\"Object Detection\",frame)\n",
    "    if cv2.waitKey(2) & 0xFF==ord(\"q\"):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \".\\\\videos\\\\british_highway_traffic.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)  # If not, try capturing from the default camera\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video\")\n",
    "\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop if there are no more frames\n",
    "\n",
    "    ClassIndex, confidence, bbox = model.detect(frame, confThreshold=0.55)\n",
    "    if len(ClassIndex) != 0:\n",
    "        for ClassInd, conf, boxes in zip(ClassIndex.flatten(), confidence.flatten(), bbox):\n",
    "            if ClassInd <= 80:  # Assuming you have 80 classes\n",
    "                cv2.rectangle(frame, boxes, (255, 0, 0), 1)\n",
    "                cv2.putText(frame, classLabels[ClassInd - 1], (boxes[0] + 10, boxes[1] + 40), font,\n",
    "                            fontScale=font_scale, color=(0, 255, 0), thickness=2)\n",
    "    cv2.imshow(\"Object Detection\",frame)\n",
    "    if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ft2font\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv2.dnn_DetectionModel(frozen_model,config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setInputSize(320,320)\n",
    "model.setInputScale(1.0/127.5)\n",
    "model.setInputMean((127.5,127.5,127.5))\n",
    "model.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65ac1c1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Function to dehaze image using wavelet transform\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdehaze_image_wavelet\u001b[39m(image):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Convert the image to grayscale\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pywt'"
     ]
    }
   ],
   "source": [
    "#dehazing and object detection\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# Function to dehaze image using wavelet transform\n",
    "def dehaze_image_wavelet(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply wavelet transform\n",
    "    coeffs2 = pywt.dwt2(gray, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    \n",
    "    # Process the detail coefficients\n",
    "    LH = np.where(LH > np.mean(LH), np.mean(LH), LH)\n",
    "    HL = np.where(HL > np.mean(HL), np.mean(HL), HL)\n",
    "    HH = np.where(HH > np.mean(HH), np.mean(HH), HH)\n",
    "    \n",
    "    # Reconstruct the image\n",
    "    coeffs2 = LL, (LH, HL, HH)\n",
    "    dehazed_gray = pywt.idwt2(coeffs2, 'haar')\n",
    "    \n",
    "    # Convert the grayscale dehazed image back to BGR\n",
    "    dehazed_image = cv2.cvtColor(dehazed_gray.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    return dehazed_image\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(image, net, output_layers, classes):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Create a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Perform the forward pass\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # Initialize lists to hold detection data\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Process each output\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply non-max suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    # Draw bounding boxes with labels\n",
    "    for i in indices.flatten():\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        color = (0, 255, 0)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        text = f'{label}: {confidence:.2f}'\n",
    "        font_scale = 0.75  # Adjusted font scale for better readability\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)\n",
    "        \n",
    "        # Draw background rectangle for text\n",
    "        cv2.rectangle(image, (x, y - text_height - 10), (x + text_width, y), color, -1)\n",
    "        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load YOLO\n",
    "def load_yolo_model():\n",
    "    yolov3weights = \"C:\\\\Users\\\\TUSHAR SAIN\\\\Desktop\\\\models\\\\Real-Time-Object-Detection-With-OpenCV-master\\\\yolov3.weights\"\n",
    "    yoloconfig = \"C:\\\\Users\\\\TUSHAR SAIN\\\\Desktop\\\\models\\\\Real-Time-Object-Detection-With-OpenCV-master\\\\yolov3.cfg\"\n",
    "    net = cv2.dnn.readNet(yolov3weights, yoloconfig)\n",
    "    with open(\"C:\\\\Users\\\\TUSHAR SAIN\\\\Desktop\\\\models\\\\Real-Time-Object-Detection-With-OpenCV-master\\\\coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    \n",
    "    # Debug print statements\n",
    "    print(\"Layer names:\", layer_names)\n",
    "    print(\"Unconnected out layers:\", unconnected_out_layers)\n",
    "    \n",
    "    # Adjusted output layers indexing\n",
    "    output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    return net, output_layers, classes\n",
    "\n",
    "def main(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(\"Error: Failed to read the image\")\n",
    "        return\n",
    "\n",
    "    # Dehaze the image\n",
    "    dehazed_image = dehaze_image_wavelet(image)\n",
    "\n",
    "    # Load YOLO model\n",
    "    net, output_layers, classes = load_yolo_model()\n",
    "\n",
    "    # Perform object detection on the dehazed image\n",
    "    detected_image = detect_objects_yolo(dehazed_image.copy(), net, output_layers, classes)\n",
    "\n",
    "    # Display images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Dehazed Image', dehazed_image)\n",
    "    cv2.imshow('Object Detection on Dehazed Image', detected_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"C:\\\\Users\\\\TUSHAR SAIN\\\\Desktop\\\\models\\\\Dehazer-main\\\\input-samples\\\\S1.jpg\"\n",
    "    main(input_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4686af77",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (__init__.py, line 144)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\TUSHAR SAIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 4\u001b[1;36m\n\u001b[1;33m    import image_dehazer\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\TUSHAR SAIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\image_dehazer\\__init__.py:144\u001b[1;36m\u001b[0m\n\u001b[1;33m    temp = np.maximum(np.minimum(temp, 255), 0)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import image_dehazer\n",
    "\n",
    "# Function to dehaze image using wavelet transform\n",
    "def dehaze_image(image_path):\n",
    "    # Read the image\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image is read successfully\n",
    "    if frame is None:\n",
    "        print(\"Error: Failed to read the image at\", image_path)\n",
    "        return\n",
    "\n",
    "    # Dehaze the image\n",
    "    corrected_image = dehaze_frame(frame)\n",
    "\n",
    "    # Save the dehazed image\n",
    "    output_path = os.path.join('output', os.path.basename(image_path))\n",
    "    cv2.imwrite(output_path, corrected_image)\n",
    "    print(f\"Dehazed image saved to {output_path}\")\n",
    "\n",
    "# Function to dehaze frame and perform object detection\n",
    "def dehaze_frame(frame):\n",
    "    # Remove haze using the image_dehazer module\n",
    "    corrected_img, _ = image_dehazer.remove_haze(frame)\n",
    "    \n",
    "    # Perform object detection using YOLO\n",
    "    net, output_layers, classes = load_yolo_model()\n",
    "    detected_image = detect_objects_yolo(corrected_img, net, output_layers, classes)\n",
    "\n",
    "    return detected_image\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(image, net, output_layers, classes, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Create a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Perform the forward pass\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # Initialize lists to hold detection data\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Process each output\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply non-max suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    \n",
    "    # Define different colors for different classes\n",
    "    class_colors = {\n",
    "        \"person\": (255, 0, 0),    # Blue\n",
    "        \"car\": (0, 255, 0),       # Green\n",
    "        \"bicycle\": (0, 0, 255),   # Red\n",
    "        # Add more classes and colors as needed\n",
    "    }\n",
    "    \n",
    "    # Draw bounding boxes with labels\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            box = boxes[i]\n",
    "            x, y, w, h = box\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            \n",
    "            color = class_colors.get(label, (0, 0, 255))  # Default color: Red\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            text = f'{label}: {confidence:.2f}'\n",
    "            font_scale = 0.75  # Adjusted font scale for better readability\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)\n",
    "            \n",
    "            # Draw background rectangle for text\n",
    "            cv2.rectangle(image, (x, y - text_height - 10), (x + text_width, y), color, -1)\n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load YOLO model\n",
    "def load_yolo_model():\n",
    "    yolov3weights = \"yolov3.weights\"\n",
    "    yoloconfig = \"yolov3.cfg\"\n",
    "    net = cv2.dnn.readNet(yolov3weights, yoloconfig)\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    \n",
    "    # Adjusted output layers indexing\n",
    "    output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    return net, output_layers, classes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"images\"\n",
    "    output_directory = \"output\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            input_image_path = os.path.join(input_directory, filename)\n",
    "            dehaze_image(input_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\TUSHAR SAIN\\Downloads\\30delhi-cold1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57450204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FNN=[66,69,71,73,75]\n",
    "CNN=[70,72,74,79,80]\n",
    "RCNN=[80,81,82,83,84]\n",
    "LSTM=[81,83,85,87,88]\n",
    "RNN_LSTN=[80,82,85,86,88]\n",
    "PROPOSED=[90,92,92.5,93.5,94]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"Accuracy(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[62,64,66,68,70]\n",
    "CNN=[65,66,68,69,74]\n",
    "RCNN=[73,76,79,81,85]\n",
    "LSTM=[75,74,79,81,83]\n",
    "RNN_LSTN=[76,78,80,82,83]\n",
    "PROPOSED=[90,91,93,95,96]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"Precision(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[52,54,55,56,58]\n",
    "CNN=[60,61,63,64,65]\n",
    "RCNN=[65,66,69,70,73]\n",
    "LSTM=[66,69,71,73,75]\n",
    "RNN_LSTN=[75,79,80,82,84]\n",
    "PROPOSED=[80,84,88,90,93]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"Recall(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[48,50,53,53,56]\n",
    "CNN=[50,52,55,57,59]\n",
    "RCNN=[56,54,60,63,62]\n",
    "LSTM=[56,58,60,63,66]\n",
    "RNN_LSTN=[60,61,63,66,68]\n",
    "PROPOSED=[73,80,83,86,90]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"True Positive(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[50,52,54,55,57]\n",
    "CNN=[52,56,60,61,64]\n",
    "RCNN=[58,60,63,66,68]\n",
    "LSTM=[60,62,64,68,71]\n",
    "RNN_LSTN=[60,64,66,70,75]\n",
    "PROPOSED=[69,74,80,85,89]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"False Positive(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[63,65,67,69,71]\n",
    "CNN=[70,72,73.5,75,78]\n",
    "RCNN=[70,72,74,79,83]\n",
    "LSTM=[76,77,80,83,84]\n",
    "RNN_LSTN=[78,79,82,84,86]\n",
    "PROPOSED=[80,83,87,89,91]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"Ground Truth(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN=[40,46,48,50,53]\n",
    "CNN=[50,52,58,59,60]\n",
    "RCNN=[52,54,56,59,61]\n",
    "LSTM=[60,62,63,66,70]\n",
    "RNN_LSTN=[65,66,70,72,76]\n",
    "PROPOSED=[67,76,81,83,89]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "width=10\n",
    "\n",
    "plt.bar(Y+width,FNN,width=10)\n",
    "\n",
    "plt.bar(Y+2*width,CNN,width=10)\n",
    "\n",
    "plt.bar(Y+3*width,RCNN,width=10)\n",
    "\n",
    "plt.bar(Y+4*width,LSTM,width=10)\n",
    "\n",
    "plt.bar(Y+5*width,RNN_LSTN,width=10)\n",
    "plt.bar(Y+6*width,PROPOSED,width=10)\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.ylabel(\"Detection Rate(%)\")\n",
    "plt.xticks(Y+width*3.5,Y)\n",
    "plt.yticks(np.arange(0,160,10))\n",
    "plt.legend([\"FNN\",\"CNN\",\"RCNN\",\"LSTM\",\"RNN-LSTN\",\"PROPOSED\"],loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ed733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "FNN=[40,46,48,50,53]\n",
    "CNN=[50,52,58,59,60]\n",
    "RCNN=[52,54,56,59,61]\n",
    "LSTM=[60,62,63,66,70]\n",
    "RNN_LSTN=[65,66,70,72,76]\n",
    "PROPOSED=[67,76,81,83,89]\n",
    "Y=np.array([100,200,300,400,500])\n",
    "\n",
    "# Transpose the data array\n",
    "data = np.array([Y,FNN, CNN, RCNN, LSTM, RNN_LSTN, PROPOSED]).T\n",
    "row_labels =[\"\",\"\",\"\",\"\",\"\"]\n",
    "col_labels = [\"Number of images\",'FNN', 'CNN', 'RCNN', 'LSTM', 'RNN_LSTN', 'PROPOSED']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "table = ax.table(cellText=data,\n",
    "                 rowLabels=row_labels,\n",
    "                 colLabels=col_labels,\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "# Hide axes\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import image_dehazer\n",
    "import numpy as np\n",
    "\n",
    "# Function to dehaze image using wavelet transform\n",
    "def dehaze_image(image_path):\n",
    "    # Read the image\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image is read successfully\n",
    "    if frame is None:\n",
    "        print(\"Error: Failed to read the image at\", image_path)\n",
    "        return\n",
    "\n",
    "    # Dehaze the image\n",
    "    corrected_image = dehaze_frame(frame)\n",
    "\n",
    "    # Save the dehazed image\n",
    "    output_path = os.path.join('output', os.path.basename(image_path))\n",
    "    cv2.imwrite(output_path, corrected_image)\n",
    "\n",
    "# Function to dehaze frame and perform object detection\n",
    "def dehaze_frame(frame):\n",
    "    # Remove haze using the image_dehazer module\n",
    "    corrected_img, _ = image_dehazer.remove_haze(frame)\n",
    "    \n",
    "    # Perform object detection using YOLO\n",
    "    net, output_layers, classes = load_yolo_model()\n",
    "    detected_image = detect_objects_yolo(corrected_img, net, output_layers, classes)\n",
    "\n",
    "    return detected_image\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(image, net, output_layers, classes, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Create a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Perform the forward pass\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # Initialize lists to hold detection data\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Process each output\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply non-max suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    \n",
    "    # Define different colors for different classes\n",
    "    class_colors = {\n",
    "        \"person\": (255, 0, 0),    # Blue\n",
    "        \"car\": (0, 255, 0),       # Green\n",
    "        \"bicycle\": (0, 0, 255),   # Red\n",
    "        # Add more classes and colors as needed\n",
    "    }\n",
    "    \n",
    "    # Draw bounding boxes with labels\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            box = boxes[i]\n",
    "            x, y, w, h = box\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            \n",
    "            color = class_colors.get(label, (0, 0, 255))  # Default color: Red\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            text = f'{label}: {confidence:.2f}'\n",
    "            font_scale = 0.75  # Adjusted font scale for better readability\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)\n",
    "            \n",
    "            # Draw background rectangle for text\n",
    "            cv2.rectangle(image, (x, y - text_height - 10), (x + text_width, y), color, -1)\n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load YOLO model\n",
    "def load_yolo_model():\n",
    "    yolov3weights = \"yolov3.weights\"\n",
    "    yoloconfig = \"yolov3.cfg\"\n",
    "    net = cv2.dnn.readNet(yolov3weights, yoloconfig)\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    \n",
    "    # Adjusted output layers indexing\n",
    "    output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    return net, output_layers, classes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"images\"\n",
    "    output_directory = \"output\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            input_image_path = os.path.join(input_directory, filename)\n",
    "            dehaze_image(input_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import image_dehazer\n",
    "module_path = inspect.getfile(image_dehazer)\n",
    "\n",
    "# Print the path\n",
    "print(\"Path to the image_dehazer module:\", module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a5878",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e52aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
